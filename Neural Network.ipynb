{
 "metadata": {
  "name": "",
  "signature": "sha256:dd318c440d6a5f10bd00936df8ccdc77c6fa223c167f75c50bb38372c5ab8be2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Neural network in action"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this little example is to show how with Python it is easy to implement a simple neural network (multilayer perceptron with backpropagation) to fit various functions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1- Namespaces declaration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2- Functions declaration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we declare and implement two useful functions. The sigmoid function is our activation function. We could have chosen the tanh one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x_input):\n",
      "    return 1 / (1 + np.exp(-x_input))\n",
      "\n",
      "\n",
      "def circle(x_in):\n",
      "    return np.sqrt(x_in[0, :]**2 + x_in[1, :]**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3- Layer class implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Layer class is a container to store the weights of each nodes and the bias of the current layer. The only function declared is to compute the output of the layer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Layer:\n",
      "    def __init__(self, nb_input, nb_output, mode):\n",
      "        # Initialize weights to random values\n",
      "        # nbNode: number of nodes in the layer\n",
      "        # dim: dimension of the input vector\n",
      "        self.nb_input = nb_input\n",
      "        self.nb_output = nb_output\n",
      "        self.bias = []\n",
      "        if mode == 'random':\n",
      "            self.weights = 1 * np.random.uniform(-1, 1, (nb_output, nb_input+1))\n",
      "        elif mode == 'ones':\n",
      "            self.weights = np.ones((nb_output, nb_input+1))\n",
      "    \n",
      "    def output(self, x_input, one_dimension=False):\n",
      "        x_input = np.array(x_input)\n",
      "        if x_input.ndim == 1 and not one_dimension:\n",
      "            x_input = x_input.reshape(-1, 1)\n",
      "        self.bias = np.ones((1, x_input.shape[1]))\n",
      "        input_vector = np.concatenate((self.bias, x_input))\n",
      "        a = self.weights.dot(input_vector)\n",
      "        z = sigmoid(a)\n",
      "        return z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4- MLP class implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's where the magic happens ! The training part of the algorithm consists to propagate forward the input signal. We then compute the error by comparison with the expected result. The error is back propagated to compute the Jacobian matrix, and finally the weigths of each layer is updated according to the gradient descent optimization algorithm. Note that we use kind of regularization here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Various parameters can be tuned : \n",
      "<ul>\n",
      "<li> The number of layers </li>\n",
      "<li> The number of nodes inside each layer </li>\n",
      "<li> The initial weights (chosen random from a uniform distribution where we control parameters) </li>\n",
      "<li> The learning rate (critical) </li>\n",
      "<li> The regularization parameter (fine tuning to overcome overfitting) </li>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP:\n",
      "    def __init__(self, topology, mode='random'):\n",
      "        # topology: an array of length equal to the number of layers.\n",
      "        # Each coordinate is the number of nodes in the layer\n",
      "        self.layers = []\n",
      "        self.layer_outputs = []\n",
      "        self.error = []\n",
      "        self.learning_rate = None\n",
      "        self.one_dimension = topology[0] == 1\n",
      "        for input_dim, output_dim in zip(topology[:-1], topology[1:]):\n",
      "            self.layers.append(Layer(input_dim, output_dim, mode))\n",
      "\n",
      "    def train(self, x_train, y_train, learning_rate=0.2):\n",
      "        x_train = np.array(x_train)\n",
      "        y_train = np.array(y_train)\n",
      "        if x_train.ndim == 1 and not self.one_dimension:\n",
      "            x_train = x_train.reshape(-1, 1)\n",
      "        if y_train.ndim == 1:\n",
      "            y_train = y_train.reshape(1, -1)\n",
      "        self.learning_rate = learning_rate\n",
      "        self.forward_propagation(x_train)\n",
      "        self.back_propagation(y_train)\n",
      "        self.update_weights()\n",
      "\n",
      "    def forward_propagation(self, x_fw):\n",
      "        x_fw = np.array(x_fw)\n",
      "        self.layer_outputs = [x_fw]\n",
      "        self.layers[0].z = x_fw\n",
      "        for layer in self.layers:\n",
      "            self.layer_outputs.append(layer.output(self.layer_outputs[-1], self.one_dimension))\n",
      "        return self.layer_outputs[-1]\n",
      "\n",
      "    def back_propagation(self, y_bw):\n",
      "        self.error = [np.concatenate((np.zeros((1, y_bw.shape[1])), self.layer_outputs[-1] - y_bw))]\n",
      "        for layer, layer_output in zip(reversed(self.layers), reversed(self.layer_outputs[:-1])):\n",
      "            layer_output = np.concatenate((layer.bias, layer_output))\n",
      "            next_error = self.error[-1][1:]\n",
      "            temp = layer.weights.T.dot(next_error) * layer_output * (1-layer_output)\n",
      "            self.error.append(temp)\n",
      "        self.error.reverse()\n",
      "\n",
      "    def update_weights(self):\n",
      "        for layer, error, layer_output in zip(self.layers, self.error[1:], self.layer_outputs[:-1]):\n",
      "            error = error[1:]\n",
      "            layer_output = np.concatenate((layer.bias, layer_output))\n",
      "            layer.weights -= self.learning_rate * error.dot(layer_output.T) \\\n",
      "                             + 0.01 * layer.weights/error.shape[1]\n",
      "\n",
      "    def output(self, x_input):\n",
      "        a = np.array(x_input)\n",
      "        if a.ndim == 1 and not self.one_dimension:\n",
      "            a = a.reshape(-1, 1)\n",
      "        for layer in self.layers:\n",
      "            a = layer.output(a)\n",
      "        return a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5- Define the test function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_function(x_in):\n",
      "    return sigmoid(x_in)\n",
      "    # return x_in**3\n",
      "    # return np.sin(x_in)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "6- And the main part is found here. That's where we test our implementation of MLP"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "6.1- Test function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    # Simulation d'une fonction\n",
      "    # seed the random generator\n",
      "    np.random.seed(100)\n",
      "    # Declare the MLP class\n",
      "    # The network is composed of two hidden layers of 4 nodes each\n",
      "    network = MLP(topology=[1, 4, 4, 1])\n",
      "    # The input range\n",
      "    x = np.arange(-10, 10.1, 0.25)\n",
      "    x = x.reshape(1, x.shape[0])\n",
      "    # The function to fit\n",
      "    y = test_function(x)\n",
      "    # As the sigmoid function returns a value in the range [0; 1], we reshape the expected result to fit inside this window\n",
      "    y_min = np.min(y)\n",
      "    y_max = np.max(y)\n",
      "    y_to_fit = 0.9 * (y - y_min) / (y_max - y_min) + 0.1\n",
      "    # Training of our network\n",
      "    for n in xrange(int(1e3)):\n",
      "        network.train(x, y_to_fit, learning_rate=0.01)\n",
      "        fit = [((network.output(i)[0, 0] - 0.1) * (y_max - y_min) + y_min) / 0.9 for i in x.T]    \n",
      "    # Plot the result\n",
      "    plt.plot(x.flatten(), y.flatten(), '.r')\n",
      "    plt.plot(x.flatten(), fit)\n",
      "    plt.ylim((y_min-0.1, y_max+0.1))\n",
      "    plt.legend(('Expected','Fit'))\n",
      "    plt.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD7CAYAAACVMATUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8VPWZx/HPQ8L9lggWlbtVRC0olQJWlKliRdAKKCBq\n1dVtXFtsu5WK0rWlsq2XuqvrHYRKqyIKarW1iqAEqBdABQQhBEGBIAWDIJcAIeHZP86YDmMgFyY5\nM5Pv+/WaFzkzZ2YewvDNL8/5nd8xd0dERNJDvbALEBGRxFGoi4ikEYW6iEgaUaiLiKQRhbqISBpR\nqIuIpJHM2nojM9PcSRGRanB3q+y+tTpSd3fdEnT7zW9+E3oN6XLT91Lfz2S+VZXaLyIiaUShLiKS\nRhTqKSoSiYRdQtrQ9zKx9P0Ml1WnZ1OtNzLz2novEZF0YWZ4FQ6U1trsFxFJXmaVzgypQYkY+CrU\nRQRITKBI9SXqB6t66iIiaUShLiKSRhTqIiJpRKEuIlILpkyZwtlnn13j76NQF5Gk1alTJ5o0aULz\n5s3Lbj/96U9DqSUSiTB58uRQ3rsqNPtFRJKWmfG3v/2Nc889N+xSUmbaZ4UjdTP7o5ltNrNlh9nn\nATNbbWZLzaxHYksUETnYjTfeyGWXXVa2PWbMGPr37w9Abm4u7dq148477+Too4+mc+fOTJ06tWzf\nffv2MXr0aDp27MgxxxzDjTfeyN69e8sef+mllzj99NNp2bIlJ5xwAjNnzuRXv/oV8+fPZ9SoUQf9\ntpCXl8f5559Pq1at6Nq1K9OnTy97na1bt/KDH/yAli1b0rt3b9asWVPT35ZAJVYIOxvoASw7xOMD\ngb9Hv+4NvHuI/VxEktNh/3/+6Efu/fq5X3ih+7Zt1XuDar5Gp06dfPbs2V+7v6ioyLt06eJTpkzx\nefPmeevWrX3jxo3u7j5nzhzPzMz0m2++2YuLi33u3LnetGlTX7Vqlbu7//znP/dLLrnEt23b5jt3\n7vSLL77Yb7vtNnd3X7Bggbds2bLsPTdu3Oh5eXnu7h6JRHzy5MllNezatcvbtWvnU6ZM8dLSUl+8\neLG3bt3aV6xY4e7uI0aM8BEjRnhRUZEvX77c27Zt62efffYh/66H+jeI3l/5VR0rtRN0OkyoPwaM\niNnOA9qUs98h/zIiEq7D/v/s1y+ICnAfNqx6b1DN1+jYsaM3a9bMs7Kyym6TJk1y9yCAs7OzvWPH\njj5t2rSy53wV6kVFRWX3DR8+3MePH+8HDhzwpk2b+po1a8oee/vtt71z587u7p6Tk+O/+MUvyq0l\nEomUvbe7+7Rp074W0jk5Of7b3/7WS0pKvH79+mU/SNzdx44d63379j3k3zVRoZ6InnpbYEPMdgHQ\nDticgNcWkbA1aRL82bMnTJxYq69hZrz00kvl9tR79erF8ccfT2FhIcOGDTvosezsbBo3bly23bFj\nRzZt2kRhYSFFRUWcccYZZY+5OwcOHACgoKCAQYMGHbaer6xbt44FCxaQnZ1ddl9JSQlXX301hYWF\nlJSU0L59+7LHOnToUPFfOCcH8vOD79fUqXDLLRU/J06iDpTGH0Eo93zjcePGlX0diUS0mptIKpg6\nNQibiRMhKyu814jz8MMPU1xczHHHHcc999zDrbfeWvbYtm3bKCoqokn0h8m6devo3r07rVu3pnHj\nxqxYsYJjjz32a6/Zvn17Pv7443LfL/5AaYcOHejXrx+vv/761/YtLS0lMzOT9evXc9JJJwGwfv36\nCv9OeUv2snDRWhZQBD0HQNGeCp/zNZUZzlNx++XymG21X0RSTLL+/zxUT33VqlWenZ3tH374oa9e\nvdqzs7N9yZIl7v6v9svo0aO9uLjY582bd1BP/Wc/+5kPHz7ct2zZ4u7uBQUFPnPmTHd3X7hwoWdl\nZfkbb7zhpaWlXlBQUNZTv/zyy33s2LFlNezcudM7duzoTz75pBcXF3txcbEvXLjQV65c6e5BT/3y\nyy/3oqIi/+ijjyrVU+/SdIOfxEo/qdEn3rVLiXdtui6UnnrsgdI+6ECpSMpJ1v+fnTp18saNG3uz\nZs3KbkOGDPFevXr53XffXbbfo48+6t26dfPi4mKfM2eOt2vXzn/3u99569atvWPHjv7UU0+V7bt3\n714fO3asH3/88d6iRQs/+eST/cEHHyx7/MUXX/Tu3bt78+bN/YQTTvDXX3/d3d3feecd79Kli2dn\nZ/vPfvYzdw9+uAwaNMiPPvpob9WqlZ933nm+dOlSd3f//PPP/aKLLvIWLVp47969/fbbb6/4QOm2\nbcExh68OJm/bVuVQr3A9dTN7BugHtCbok/8GqB9N6QnRfR4CBgC7gX9z9w/KeR2v6L1EpPb4j3L4\n5/JCVvsJ9FvwB9Ll/2dubi4//OEP2bBhQ8U7J5HouumHuj9x66m7+8hK7DOqsm8oIrXPHVauhPfe\ngyVLorf595JRso8u5AN/CLtESRCdUSqShtxh2TKYMwfmzQtuzfd9Tu9GH9LjqHXc+vvhnGb/QZs3\nn4GePbH3wq44sVLl7M+aoMvZiaSJAwdgwQJ44YXgdmBLId9v/jb9vpHH2U/dQPtRl8DcucHOw4YF\nM1GiM1IsOztt2i+pKlHtF4W6SIrbvDnI54kToUULGDo0uJ3+8wg2LybEd+2CV18N5orPmnXQ1MJD\nBYrUnkSFulZpFElRCxbAVVdB166wYQP8rdcdfHR0hPHvD6RH5+1Y07gTfqZODcI9LtAlvWikLpJi\nVq6EX/4Sli+Hm26C666D7GwgEjlke6WiENdIPXwaqYvUMYWFMGoUnHMOnFv4HKs6nM/Nbwwk27YH\nO8Sfip+VBc89p1F5HaNQF0ly7jBlCpx8MtSrF4zUf9HoERrOnx30yHNygh3VXhE0pVEkqe3aBT/+\nMbz/fjA98Vvfij5Q3gJZX43M05i706JFC5YtW0anTp3CLicpaaQukqSWLg0yu0EDWNR7FN8aFYGB\nA2H79jozKo+/nF3Lli1ZvXp1WaBfe+213H777eEWmWQU6iJJ6KmnoH9/uP12mDQJmqxdHhwE/ard\nUkf65V9dzm7nzp3s3LmTHTt2cMwxx4RdVlJT+0UkyTz+OPz2t0GGn3JK9M5ErGmeJurVq8fq1at5\n4403mDp1KmbG/fffz7nnnstLL70UdnmhU6iLJJGHH4Z77gn65yeeGPNADaxHnioONc0vJyeHd955\nh/bt23PHHXeEUFlyUqiLJIn77oMHHoDcXOh8Z9wVcEI+CJqopVSqOhXe3Rk8eDCZmUFUlXdhHc2v\nP5hCXSQJ3HdfMEqfOxc6dCAI9K9OJMrJCX1WS1i5Wd7l7OrV06HAw1Goi4TstdfgD38ITvsvu6Sl\neuiVUpdXYzwU/cgTCdHatXDNNfDsszGBDnVmyuKRatOmDWvXrg27jKSiUBcJye7dMGQI/Nd/wdlP\n5gRrt3w1D72OTFmsjtjR+fXXX8+KFSvIzs5m6NChIVaVPLSgl0gI3IMVFuvVgz//Gex7kYMX46rl\nHroW9ApfrV3OTkQS74EHYMUKeOut6MwS9dAlQTRSF6lleXnQty8sWgSdO0fv3L491HnoGqmHT1c+\nEklB7kHr/LLLgrXQk4VCPXxaT10kBU2ZAkVFwcqLIjVBI3WRWvL558HSua+9Bj0eLeeM0RBppB4+\njdRFUszo0cGMlx49+NcZo7EXuRBJAM1+EakFc+YEa7p89FH0jiSc7aKzM9OD2i8iNay4GLp1g3vv\nhYsvjt4Z8mwXSR2a/SKSZCZMgBdegJkzw65EUpFCXSSJ7NsXrIs+fTr07h12NZKKEn6g1MwGmFme\nma02szHlPN7azF4zsyVmttzMrq1izSJpa/Jk6MYyeo+J/GtdF5EadNiRupllAKuA/sBGYBEw0t1X\nxuwzDmjo7reZWevo/m3cvSTutTRSlzpl71444QT4S5sb6PlB9GBoCOu6SGpL9Ei9F/Cxu3/q7vuB\nacAlcftsAlpEv24BbI0PdJG6aOJEOOMM6NlmQ3BHEs10kfRV0ZTGtsCGmO0CIL4z+Djwppl9BjQH\nhieuPJHUtGcP3HUXvPIK0LnuXl9Ual9FoV6ZfslYYIm7R8zsm8AsMzvN3XfG7zhu3LiyryORSLnX\nGxRJB489Bn36RE80Itzri0pqyc3NJTc3t9rPr6in3gcY5+4Dotu3AQfc/e6Yff4O/M7d34puvwGM\ncff34l5LPXWpE3bvDnrpM2dC9+5hVyOpLtE99feAE82sk5k1AEYAL8ftk0dwIBUzawOcBOj6UlJn\n/elPwShdgS5hOGz7xd1LzGwUMBPIACa7+0ozuyH6+ATg98ATZraU4IfELe7+RQ3XLZKU3OGhh+CR\nR8KuROoqnXwkkkCzZ8N/Dt/Ih92uxJomxwqMktq0SqNIiB58EG5q/Qw2TyswSjgU6iIJ8sknwTVH\nr+z0VnCH5qVLCBTqIgnyyCNw7bXQ9LkngjNHZ81S60VqnXrqIglQVAQdOsDChXD88WFXI+lEPXWR\nEDz9NHz3uwp0CZ9CXeQIuUcPkN4UdiUiCnWRIzZvHuzfD/37h12JiEJd5IhN+vd3uWH/Q9ggrZcu\n4dOBUpEjsGMHdDhqF6tLO3M0hVovXRJOB0pFatGMGRBptSwIdM1LlySgUBc5AlOmwLX/213z0iVp\nqP0iUk1r1sCZZ0JBATRoEHY1kq7UfhGpJX/+M4wcqUCX5KKRukg1HDgA3/wmvPDCV1c3EqkZGqmL\n1IJ586B5czj99LArETmYQl2kGqZMCRbvskqPn0Rqh9ovIlW0axe0bw95edCmTdjVSLpT+0Wkhj1/\n0RP0rfc2bf5NZ5BK8lGoi1TRU0u7cfUX9+nKRpKU1H4RqYLNm+Gkdrv4rOQbNOl5qk44khpX1fZL\nZk0WI5Junn8eBg1uQBO7KFgSQIEuSUYjdZEqOOcc+OUv4eKLw65E6oqqjtQV6iKVVFAAp50Gn30G\nDRuGXY3UFZr9IlJDnnsOBg9WoEtyU6iLVNKzz8KIEWFXIXJ4ar+IVMLatdCnT9B6ydT0AqlFar+I\n1IBnn4XLLlOgS/JTqItUJCeHab/7mMs/uEVnkErSqzDUzWyAmeWZ2WozG3OIfSJmttjMlptZbsKr\nFAnRysV7KdzdmL4L7tUZpJL0DvvLpJllAA8B/YGNwCIze9ndV8bskwU8DFzg7gVm1romCxapbc9u\nv4DhPEe9nmfoGqSS9CoaqfcCPnb3T919PzANuCRunyuA5929AMDdCxNfpkg43GGaXc6Icwu1JICk\nhIpCvS2wIWa7IHpfrBOBo8xsjpm9Z2Y/TGSBImH66CMo2ptB79m/U6BLSqjoWH5l5iDWB74NnAc0\nAd4xs3fdffWRFicStunTg1kvuhiGpIqKQn0j0D5muz3BaD3WBqDQ3fcAe8xsHnAa8LVQHzduXNnX\nkUiESCRS9YpFatGMGTBpUthVSF2Sm5tLbm5utZ9/2JOPzCwTWEUwCv8MWAiMjDtQ2pXgYOoFQENg\nATDC3VfEvZZOPpKUsmIFfP/7sH491NPkXwlJQpfedfcSMxsFzAQygMnuvtLMbog+PsHd88zsNeBD\n4ADweHygi6SiGTOC1osCXVKJlgkQOYRu3eDRR6Fv37ArkbpMywSIJEBeHmzdCt/9btiViFSNQl2k\nHDNmwKWXqvUiqUcfWZF4OTnMuOtjhi3SWi+SehTqInFWL9nN5t1NOWvB/2itF0k5CnWRONO/PJ+h\nvEBGz29rrRdJOQp1kTgzGl3FsH6fa60XSUma0igSY82aYMbLZ59BRkbY1YhoSqPIEZk+HYYOVaBL\n6lKoi8SYPh2GDw+7CpHqU6iLRK1ZAwUFcM45YVciUn0KdZEotV4kHSjURaKmT4dhw8KuQuTIKNRF\ngLVr1XqR9FDRRTJE0ltODuTnM33T1QwZeCWZmQ3DrkjkiGikLnVbfj7Mncv0/O4MW3t32NWIHDGF\nutRtTZqwls6szzyefs//NOxqRI6Y2i9St02dyvRzXmHIt5uS2VqtF0l9GqlL3ZaVxfQGVzLsKgW6\npAet/SJ12tq10Ls3bNoEmfq9VZKQ1n4RqYJp04K56Qp0SRcKdanTpk6FkSPDrkIkcRTqUmctWwZf\nfglnnRV2JSKJo1CXOuuZZ4JRui4uLelEnUSpk9yDUH/hhbArEUksjVGkTnr3XWjUCE4/PexKRBJL\noS51S04ORCJMHfkyI4fswSo9UUwkNSjUpW7Jz6dk7j+Yvq4XIxePCbsakYRTqEvd0qQJc/ge7Zt8\nwYnP3BF2NSIJV2Gom9kAM8szs9VmdsihjZl9x8xKzGxoYksUSaCpU5naaSwjx3aGrKywqxFJuMMu\nE2BmGcAqoD+wEVgEjHT3leXsNwsoAp5w9+fLeS0tEyCh27sXjj0Wli+Htm3DrkakYoleJqAX8LG7\nf+ru+4FpwCXl7HcTMAP4vNKVioTglVeCGS8KdElXFYV6W2BDzHZB9L4yZtaWIOgfjd6l4bgkrSlT\n4Jprwq5CpOZUFOqVCej7gVujvRWL3kSSzqZN8I9/wGWXhV2JSM2p6IzSjUD7mO32BKP1WGcA0yyY\n8NsauNDM9rv7y/EvNm7cuLKvI5EIkUik6hWLVNOTT8LQodCsWdiViBxabm4uubm51X5+RQdKMwkO\nlJ4HfAYspJwDpTH7PwH81d2/dvK1DpRKmNzhlFPg8cehb9+wqxGpvKoeKD3sSN3dS8xsFDATyAAm\nu/tKM7sh+viEI6pWpJa8+y6UlmpFRkl/uvKRpLecHMjPJ2ftGDpfG+G2OxqHXZFIlejKRyKx8vMp\nmruQGRt6c/Xin4ddjUiNU6hLemvShOe5lD4tVtL2ybvDrkakxmk9dUlvU6fyRJcCbrynM2Q1Dbsa\nkRqnnrqktU8+ge98BzZuhIYNw65GpOrUUxeJ8cQTwSXrFOhSV2ikLmmruBg6doQ33gjmqIukIo3U\nRaKmT4dTT1WgS92iUJe09cADcNNNYVchUrsU6pKWFi6ELVvgoovCrkSkdinUJS09+CD85CeQkRF2\nJSK1SwdKJb3k5LB5+ed0XfRn1uQf4KjOLcOuSOSI6ECp1G35+Ux851sMK3mGo8b8KOxqRGqdQl3S\nyv5GzXmM/+CmU96EiRPDLkek1inUJa28MHwaJx69nW5vPQZZWWGXI1LrtPaLpA13uG9iU2557FRQ\nnksdpZG6pI3Zs2HHDhg8OOxKRMKjUJe0MX48/OpXUE+faqnD9PGXtDB3LmzaBCNGhF2JSLgU6pIW\nxo+HsWMhU0eJpI7TyUeS2nJyeGdRJles+jX56xpR/2gdIZX0opOPpG7Jz2f8kou4dc9vqP+TnLCr\nEQmdQl1S2nv7T2MZ3bj228t0spEIar9Iihs8aD/nffYkN80ZqpONJC1Vtf2iUJeUNX8+XHklrFoF\njRuHXY1IzVBPXeqEAwfgP/8T7rpLgS4SS6EuKenJJ6F+/eCi0iLyL2q/SMrZtQu6doUZM6BPn7Cr\nEalZar9I2rvnHujXT4EuUh6dfyepIyeH9R9u5+EPJrH4fQd0VSOReJUaqZvZADPLM7PVZjamnMev\nNLOlZvahmb1lZt0TX6rUefn53LbgEn6y/346jNdVjUTKU+FI3cwygIeA/sBGYJGZvezuK2N2Wwuc\n4+5fmtkAYCKgX44lod7ccybzOIcJPSbAxJfDLkckKVVmpN4L+NjdP3X3/cA04JLYHdz9HXf/Mrq5\nAGiX2DKlrtu5E67/538zoe9TNHvzZZ1oJHIIlQn1tsCGmO2C6H2Hcj3w9yMpSiTeLbfAuf0zGDj/\nNgW6yGFU5kBppechmtn3gOuAs8p7fNy4cWVfRyIRIpFIZV9a6rDZs+GVV2DZsrArEal5ubm55Obm\nVvv5Fc5TN7M+wDh3HxDdvg044O53x+3XHXgBGODuH5fzOpqnLlW2Ywd07w4TJsAFF4RdjUjtq4l5\n6u8BJ5pZJzNrAIwADjpKZWYdCAL9qvICXaRacnIY3fWvnF/6Ghf03h52NSIpocL2i7uXmNkoYCaQ\nAUx295VmdkP08QnAr4Fs4FEzA9jv7r1qrmypC/76ditmburOMrpBzgB47rmwSxJJelomQJJSfj70\n7badl4ov5MyeJTBrlg6QSp2kZQIk5e3cCUOGwPi7GnDmsPYKdJEq0Ehdkoo7DB8eZPjEiWCVHp+I\npKeqjtS19osklXvugfXr4amnFOgi1aFQl6Tx6qvwf/8HCxdCw4ZhVyOSmtRTl6Twj3/A1UN3MqPN\nT2iXMxC2awqjSHWopy6h++ADGDAAnj7mZs5f9r/BncOGaQqjCJr9IilmxQoYNCg4Y/T8dtGFP3v2\nDI6SikiVaaQuoVm7NriC0Z13wlVXEbRccnKCQNcURhGg6iN1hbqEYtkyGDgQxo6FG28MuxqR5KX2\niyS93Fw47zy458THufHZSJDuOjAqkhAKdalVzz4bnFw0bRqMPPA0zJ0bzGXMyQm7NJG0oFCXWuEO\n994Lo0cH66Ofey7QpEnwoA6MiiSMeupS47Zvh+uvh08+gb/8BTp0iHlAB0ZFDksHSiWpLFoEI0YE\nbfN7i35Mo7UrghH61KkKcpFK0IFSSQoHDgSn/A8aFKzn8tBDBIGuHrpIjdLaL5JwK1YEmV1aCu+8\nA9/8ZvQB9dBFapxG6pIwe/fC7bcHJxSNHBms51IW6BC0XIYN0/roIjVIPXU5Yu7w178GM1u6dYMH\nHoC2bQmG6/n56qGLHAGtpy61at48uPXW4GpF990X9NDL5OcHPXQIAl4LdInUOIW6VMvChTBuHKxc\nCXfcAVdcARkZcTuphy5S69R+kUo7cCBos9x7b3B1otGjgwF4w4aU32rRPHSRI6Z56pJwW7fC008H\n0xKzsuDmm+HSSyEz9ve8SORfrRathS6SMOqpS0KUlgan8//xj/Daa0GvfNIkOPvs6LVD40fmarWI\nJAWFupQpLQ2mIT7/fHA79li47jp47DHIzo7bOf4g6NSparWIJAGFeh23cye8+Sb8/e/w0ktBkF96\naTBKP/nkmB0rGplnZanlIpIEFOp1THExvP9+sKb5zJnB1336BNcIfeutuJOFYmlkLpISdKA0zRUW\nwnvvBafrz58fLLB1wglwzjlwwQXB2Z9Nm5bzxPiR+RVXBGu29OypM0JFapFmv9RR7rBxY3CZuA8/\nhA8+CAJ861Y44wzo3Ts4yPnd7x4ij+NDfPDgg2ezTJyokblICBIe6mY2ALgfyAAmufvd5ezzAHAh\nUARc6+6Ly9lHoZ4AxcWwbl2Qv3l5sGpV8Ofy5dCgAXTvHpyqf/rp8J3vQJcuUC9+hZ/y5pTHT0nc\ntUsjc5EkkNApjWaWATwE9Ac2AovM7GV3Xxmzz0DgBHc/0cx6A48CfapVvbBnD3z2WXByT+xt7VpY\nswY2bYLjjoOTToKuXaFHD7j8cjj1VGjTJvoiOTkwOR+eiYb2LbccHOLlnb5f3pREjcxFUs5hR+pm\ndibwG3cfEN2+FcDd74rZ5zFgjrs/G93OA/q5++a416qTI3V32L076G1v3Rr8uWULbN4c3LZsgX/+\nMwjyjRth95f7Oa5BIR0aF9Lh/JPokD+bDrtXcnzWFxw/YQwdvtWC+j+JG2nHh3Z862TLlopH4Tr7\nUyQpJfrko7bAhpjtAqB3JfZpB2wm3sCB5YdQ/HZW1tdbBBU9p6rblXiP/aNvY3feBnbVz2b3PQ+z\n6/cPsPOTQnZmZLHzP37Jjj+9yI7PdvFlvSy+/N4QvtzbkO3zlrJtez2+KG3Jtmbt2FZYSoaX0LrB\nTlqfdBStCvP5xv6NfKPJLtpcM4BT102jzdaVtG25i+P+cietrrsEmzcX9gI+DFpugSXRQL5zdTCq\njh9px4Z2Ts7XR91XXHHw9lf7xQa4piSKpIWKQr2yQ+v4nyLlPm/cq6/CWWfhu3ZzzvoN9CWTkutu\nonTLVkrfWkoJmZT88BZKHplIydIdlCzcSAmZ7B/+35RsdfZ/sIf9lFAy+H72f9Gc/cuaUEwD9l/4\nR4q3d6A4bz/FNGDf916keEcP9q09mn00ZF+fuezb9X32bezBPhqyp/tH7D3jLPbOvZ492/awh8bs\n6fQle4p/TdEeYzdNKTqqGaX+EE3ZTXN20uzcXTRlOM12/ZMW7KDF+MU0L2lBi02baEkBxzQvIusX\n19Fy8bNkr5/PUXxB9vf6kr39Exr/Y1YQ0l2GQVZMAK8YBmyB5dHt2wqhaSUCuaqhXd70QwW4SFLK\nzc0lNze32s+vqP3SBxgX0365DTgQe7A02n7Jdfdp0e1Dtl8ybT+lZOBu1KOUTCslo1F9Mor3kFFa\nTP1MJ7NVSzLr1yNj62bq79lBZqNMMjt3ILPgU+rv3Er9pg3JPO1U6q9aRoOtm6if1ZQG/c6k/sK3\nqb9pHQ1bt6DhkIE0mPUKDT/No+FxrWj471fT4LmnaJi3hMad2tBo7M00atWURr//NY3fn0/jrp1o\nPPH/aPxfN9Nk3qs0Pb0LTV57gQbXXoG9FtOmiJ/WV940v4EDD79PRdtwcACX1xaJv0+tE5G0VdX2\nC+5+yBvBSH4N0AloACwBTo7bZyDw9+jXfYB3D/Favm/zNi8pcT/wxTb3YcPct21zdw/+jN0u775E\nbyfra4qIxAhi+tA5HX+rzJTGC/nXlMbJ7n6nmd0Q/YEwIbrPQ8AAYDfwb+7+QTmv4xW9l4iIHEwn\nH4mIpJGqhrouPC0ikkYU6iIiaUShLiKSRhTqIiJpRKEuIpJGFOoiImlEoS4ikkYU6iIiaUShLiKS\nRhTqIiJpRKGeoo5kaU45mL6XiaXvZ7gU6ilK/3ESR9/LxNL3M1wKdRGRNKJQFxFJI7W69G6tvJGI\nSJpJyvXURUSk5qn9IiKSRhTqIiJppEZD3cyGmdlHZlZqZt+Oe+w2M1ttZnlm9v2arCMdmdk4Mysw\ns8XR24Cwa0pFZjYg+hlcbWZjwq4n1ZnZp2b2YfQzuTDselKJmf3RzDab2bKY+44ys1lmlm9mr5tZ\nVkWvU9Mj9WXAEGBe7J1mdgowAjiF4ILVj5iZfmuoGgf+1917RG+vhV1QqjGzDOCri6afAow0s5PD\nrSrlORDpQOm+AAACDklEQVSJfiZ7hV1MinmC4LMY61Zglrt3Ad6Ibh9WjQapu+e5e345D10CPOPu\n+939U+BjQB+Aqqv0EXEpVy/gY3f/1N33A9MIPptyZPS5rAZ3nw9si7v7B8Cfol//CRhc0euENTo+\nDiiI2S4A2oZUSyq7ycyWmtnkyvxaJl/TFtgQs63P4ZFzYLaZvWdmPwq7mDTQxt03R7/eDLSp6AmZ\nR/qOZjYLOKach8a6+1+r8FKaWxnnMN/bXwGPAndEt8cD/wNcX0ulpQt95hLvLHffZGZHA7PMLC86\nApUj5O5emfN9jjjU3f38ajxtI9A+Zrtd9D6JUdnvrZlNAqryA1QC8Z/D9hz8G6RUkbtviv75uZm9\nSNDiUqhX32YzO8bd/2lmxwJbKnpCbbZfYvtsLwOXm1kDM+sMnAjoSHkVRP+BvzKE4KC0VM17wIlm\n1snMGhAcvH855JpSlpk1MbPm0a+bAt9Hn8sj9TJwTfTra4C/VPSEIx6pH46ZDQEeAFoDr5jZYne/\n0N1XmNlzwAqgBPix69TWqrrbzE4naCF8AtwQcj0px91LzGwUMBPIACa7+8qQy0plbYAXzQyCbHna\n3V8Pt6TUYWbPAP2A1ma2Afg1cBfwnJldD3wKDK/wdZSlIiLpQ3PDRUTSiEJdRCSNKNRFRNKIQl1E\nJI0o1EVE0ohCXUQkjSjURUTSiEJdRCSN/D/1w4Q7aSrFEgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xb549c18>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "6.2- XOR function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    # Simulation d'une fonction XOR\n",
      "    network = MLP(topology=[2, 2, 4, 1])\n",
      "    X = np.array([[0, 1, 0, 1], [0, 0, 1, 1]])\n",
      "    y = np.array([0, 1, 1, 0])\n",
      "    for n in xrange(int(1e3)):\n",
      "        network.train(X, y, learning_rate=0.9)\n",
      "    print 'Fonction XOR'\n",
      "    print 'Input\\tOutput\\tQuantized'\n",
      "    for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
      "        print '{}\\t{:.4f}\\t{}'.format(i, network.output(i)[0, 0], 1*(network.output(i)[0] > .5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fonction XOR\n",
        "Input\tOutput\tQuantized\n",
        "[0, 0]\t0.0180\t[0]\n",
        "[1, 0]\t0.9831\t[1]\n",
        "[0, 1]\t0.9856\t[1]\n",
        "[1, 1]\t0.0142\t[0]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "6.3- OR function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    # Simulation d'une fonction OR\n",
      "    network = MLP(topology=[2, 2, 4, 1])\n",
      "    X = np.array([[0, 1, 0, 1], [0, 0, 1, 1]])\n",
      "    y = np.array([0, 1, 1, 1])\n",
      "    for n in xrange(int(1e3)):\n",
      "        network.train(X, y, learning_rate=0.9)\n",
      "    print 'Fonction OR'\n",
      "    print 'Input\\tOutput\\tQuantized'\n",
      "    for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
      "        print '{}\\t{:.4f}\\t{}'.format(i, network.output(i)[0, 0], 1*(network.output(i)[0] > .5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fonction OR\n",
        "Input\tOutput\tQuantized\n",
        "[0, 0]\t0.0218\t[0]\n",
        "[1, 0]\t0.9912\t[1]\n",
        "[0, 1]\t0.9912\t[1]\n",
        "[1, 1]\t0.9983\t[1]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}